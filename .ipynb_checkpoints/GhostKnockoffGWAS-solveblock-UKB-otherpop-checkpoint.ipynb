{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c59fd0c",
   "metadata": {},
   "source": [
    "# Run `solveblock` on UKB Indian/African/Chinese/Caribbean\n",
    "\n",
    "1. Start with UKB array data filtered to Indian samples, **on SNPs with MAF>0.01**\n",
    "2. Use LD blocks defined by `snp_ldsplit`\n",
    "3. Use covariates to adjust LD matrix following [Pan-UKB documentation](https://pan-dev.ukbb.broadinstitute.org/docs/ld#ld-matrices)\n",
    "    + sex\n",
    "    + age\n",
    "    + age^2\n",
    "    + age*sex\n",
    "    + age^2*sex\n",
    "    + first 10 PCs\n",
    "\n",
    "4. Run `solveblock` on given PLINK file\n",
    "\n",
    "```shell\n",
    "ml julia/1.10 R/4.0.2\n",
    "export OPENBLAS_NUM_THREADS=1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a5ef60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_job_names (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "using DelimitedFiles\n",
    "\n",
    "# helper function to submit 1 job to run 1 command\n",
    "function submit(command::String, ncores::Int, total_mem::Number, \n",
    "        joblog_dir::String=\"/oak/stanford/groups/zihuai/solveblock/joblogs\"; \n",
    "        jobname=\"submit\", waitfor=Int[], verbose=true, highp=false)\n",
    "    mem = round(Int, total_mem / ncores) # memory per core\n",
    "    filename = \"$jobname.sh\"\n",
    "    open(filename, \"w\") do io\n",
    "        println(io, \"#!/bin/bash\")\n",
    "        println(io, \"#\")\n",
    "        println(io, \"#SBATCH --job-name=$jobname\")\n",
    "        println(io, \"#\")\n",
    "        if highp\n",
    "            println(io, \"#SBATCH --time=168:00:00\")\n",
    "        else\n",
    "            println(io, \"#SBATCH --time=24:00:00\")\n",
    "        end\n",
    "        println(io, \"#SBATCH --cpus-per-task=$ncores\")\n",
    "        println(io, \"#SBATCH --mem-per-cpu=$(mem)G\")\n",
    "        if highp\n",
    "            println(io, \"#SBATCH --partition=candes,zihuai\")\n",
    "        else\n",
    "            println(io, \"#SBATCH --partition=candes,zihuai,normal,owners\")\n",
    "        end\n",
    "        println(io, \"#SBATCH --output=$(joinpath(joblog_dir, \"slurm-%j.out\"))\")\n",
    "        println(io, \"\")\n",
    "        println(io, \"#save job info on joblog:\")\n",
    "        println(io, \"echo \\\"Job \\$JOB_ID started on:   \\\" `hostname -s`\")\n",
    "        println(io, \"echo \\\"Job \\$JOB_ID started on:   \\\" `date `\")\n",
    "        println(io, \"\")\n",
    "        println(io, \"# load the job environment:\")\n",
    "#         println(io, \"module load julia/1.9\")\n",
    "#         println(io, \"module load biology plink/1.90b5.3\")\n",
    "#         println(io, \"module load R/4.0.2\")\n",
    "#         println(io, \"export OPENBLAS_NUM_THREADS=1\")\n",
    "#         println(io, \"export JULIA_DEPOT_PATH=\\\"/home/groups/sabatti/.julia\\\"\")\n",
    "        println(io, \"\")\n",
    "        println(io, \"# run code\")\n",
    "        println(io, \"echo \\\"$command\\\"\")\n",
    "        println(io, \"$command\")\n",
    "        println(io, \"\")\n",
    "        println(io, \"#echo job info on joblog:\")\n",
    "        println(io, \"echo \\\"Job \\$JOB_ID ended on:   \\\" `hostname -s`\")\n",
    "        println(io, \"echo \\\"Job \\$JOB_ID ended on:   \\\" `date `\")\n",
    "        println(io, \"#echo \\\" \\\"\")\n",
    "    end\n",
    "    # submit job and capture job ID\n",
    "    io = IOBuffer()\n",
    "    if length(waitfor) != 0\n",
    "        run(pipeline(`sbatch --dependency=afterok:$(join(waitfor, ':')) $filename`; stdout=io))\n",
    "    else\n",
    "        run(pipeline(`sbatch $filename`; stdout=io))\n",
    "    end\n",
    "    msg = String(take!(io))\n",
    "    verbose && print(stdout, msg)\n",
    "    jobid = parse(Int, strip(msg)[21:end])\n",
    "    # clean up and return job ID\n",
    "    close(io)\n",
    "    rm(filename, force=true)\n",
    "    return jobid\n",
    "end\n",
    "\n",
    "# helper function to submit 1 job to run multiple commands\n",
    "function submit(commands::Vector{String}, ncores::Int, total_mem::Number, \n",
    "        joblog_dir::String=\"/oak/stanford/groups/zihuai/solveblock/joblogs\"; \n",
    "        jobname=\"submit\", waitfor=Int[], verbose=true, highp=false)\n",
    "    mem = round(Int, total_mem / ncores) # memory per core\n",
    "    filename = \"$jobname.sh\"\n",
    "    open(filename, \"w\") do io\n",
    "        println(io, \"#!/bin/bash\")\n",
    "        println(io, \"#\")\n",
    "        println(io, \"#SBATCH --job-name=$jobname\")\n",
    "        println(io, \"#\")\n",
    "        if highp\n",
    "            println(io, \"#SBATCH --time=168:00:00\")\n",
    "        else\n",
    "            println(io, \"#SBATCH --time=24:00:00\")\n",
    "        end\n",
    "        println(io, \"#SBATCH --cpus-per-task=$ncores\")\n",
    "        println(io, \"#SBATCH --mem-per-cpu=$(mem)G\")\n",
    "        if highp\n",
    "            println(io, \"#SBATCH --partition=candes,zihuai\")\n",
    "        else\n",
    "            println(io, \"#SBATCH --partition=candes,zihuai,normal,owners\")\n",
    "        end\n",
    "        println(io, \"#SBATCH --output=$(joinpath(joblog_dir, \"slurm-%j.out\"))\")\n",
    "        println(io, \"\")\n",
    "        println(io, \"#save job info on joblog:\")\n",
    "        println(io, \"echo \\\"Job \\$JOB_ID started on:   \\\" `hostname -s`\")\n",
    "        println(io, \"echo \\\"Job \\$JOB_ID started on:   \\\" `date `\")\n",
    "        println(io, \"\")\n",
    "        println(io, \"# load the job environment:\")\n",
    "#         println(io, \"module load julia/1.9\")\n",
    "#         println(io, \"module load biology plink/1.90b5.3\")\n",
    "#         println(io, \"module load R/4.0.2\")\n",
    "#         println(io, \"export OPENBLAS_NUM_THREADS=1\")\n",
    "#         println(io, \"export JULIA_DEPOT_PATH=\\\"/home/groups/sabatti/.julia\\\"\")\n",
    "        println(io, \"\")\n",
    "        for command in commands\n",
    "            println(io, \"echo \\\"$command\\\"\")\n",
    "            println(io, \"$command\")\n",
    "        end\n",
    "        println(io, \"\")\n",
    "        println(io, \"#echo job info on joblog:\")\n",
    "        println(io, \"echo \\\"Job \\$JOB_ID ended on:   \\\" `hostname -s`\")\n",
    "        println(io, \"echo \\\"Job \\$JOB_ID ended on:   \\\" `date `\")\n",
    "        println(io, \"#echo \\\" \\\"\")\n",
    "    end\n",
    "    # submit job and capture job ID\n",
    "    io = IOBuffer()\n",
    "    if length(waitfor) != 0\n",
    "        run(pipeline(`sbatch --dependency=afterok:$(join(waitfor, ':')) $filename`; stdout=io))\n",
    "    else\n",
    "        run(pipeline(`sbatch $filename`; stdout=io))\n",
    "    end\n",
    "    msg = String(take!(io))\n",
    "    verbose && print(stdout, msg)\n",
    "    jobid = parse(Int, strip(msg)[21:end])\n",
    "    # clean up and return job ID\n",
    "    close(io)\n",
    "    rm(filename, force=true)\n",
    "    return jobid\n",
    "end\n",
    "\n",
    "\n",
    "\"Run a Cmd object, returning the stdout & stderr contents plus the exit code\"\n",
    "function execute(cmd::Cmd)\n",
    "    out = Pipe()\n",
    "    err = Pipe()\n",
    "\n",
    "    process = run(pipeline(ignorestatus(cmd), stdout=out, stderr=err))\n",
    "    close(out.in)\n",
    "    close(err.in)\n",
    "\n",
    "    return (\n",
    "        stdout = String(read(out)), \n",
    "        stderr = String(read(err)),  \n",
    "        code = process.exitcode\n",
    "    )\n",
    "end\n",
    "\n",
    "function get_job_names()\n",
    "    data_str, _, _ = execute(`squeue -u bbchu -h -o \"%.30j\"`)\n",
    "    lines = split(data_str, \"\\n\")\n",
    "    jobnames = String[]\n",
    "    for line in lines\n",
    "        push!(jobnames, strip(line))\n",
    "    end\n",
    "    return jobnames\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177fd6b0",
   "metadata": {},
   "source": [
    "## Obtain Indian SNP data\n",
    "\n",
    "QC of Indian genotype/phenotype was done in `ukb_phenotypes_QC_Indians.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10255b64",
   "metadata": {},
   "source": [
    "## Prepare covariate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f422baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pop in [\"indian\", \"chinese\", \"caribbean\", \"african\"]\n",
    "    file = \"/scratch/groups/sabatti/ukb_phenotypes/phenotypes.$pop.csv\"\n",
    "    df = CSV.read(file, DataFrame)\n",
    "\n",
    "    # interaction covariates\n",
    "    df[!, \"age_sex\"] = df[!, \"age\"] .* df[!, \"sex\"]\n",
    "    df[!, \"age_squared_sex\"] = df[!, \"age_squared\"] .* df[!, \"sex\"]\n",
    "\n",
    "    # sample ID is FID and IID merged\n",
    "    eid = df[!, \"eid\"]\n",
    "    df[!, \"sampleID\"] = string.(eid, \"_\", eid)\n",
    "\n",
    "    # final covariates file\n",
    "    df = df[!, vcat([:sampleID, :sex, :age, :age_squared, :age_sex, :age_squared_sex], [Symbol(\"PC$i\") for i in 1:10])]\n",
    "    CSV.write(\"/oak/stanford/groups/zihuai/solveblock/covariates.$pop.csv\", df)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78cc25",
   "metadata": {},
   "source": [
    "## Run `solveblock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a94508d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indian failed 0\n",
      "chinese failed 0\n",
      "caribbean failed 0\n",
      "african failed 0\n"
     ]
    }
   ],
   "source": [
    "obj = \"default\" # LD split objective\n",
    "exe = \"/home/groups/sabatti/.julia/dev/GhostKnockoffGWAS/app_linux_x86/bin/solveblock\"\n",
    "hg_build = 19\n",
    "\n",
    "for pop in [\"indian\", \"chinese\", \"caribbean\", \"african\"]\n",
    "    plinkfile = \"/scratch/groups/sabatti/ukb_genotypes/$pop/allchr.bed\"\n",
    "    outdir = \"/oak/stanford/groups/zihuai/solveblock/LD_files_$pop/$obj\"\n",
    "    region_dir = \"/oak/stanford/groups/zihuai/solveblock/LD_files_$pop/LD_blocks/$obj\"\n",
    "    covfile = \"/oak/stanford/groups/zihuai/solveblock/covariates.$pop.csv\"\n",
    "\n",
    "    failed = 0\n",
    "    running_jobs = get_job_names()\n",
    "    for chr in 1:22\n",
    "        ##### get quasi-independent blocks and handle SNPs falling between windows\n",
    "        region_file = joinpath(region_dir, \"chr$chr.maf0.01.thr0.01.maxr0.3.bed\")\n",
    "        df = CSV.read(region_file, DataFrame)\n",
    "        start_pos = df[!, \"start\"]\n",
    "        end_pos = df[!, \"stop\"]\n",
    "        mid_point = [floor((start_pos[i] - end_pos[i-1]) / 2) for i in 2:length(start_pos)]\n",
    "        end_pos[1:end-1] .+= mid_point\n",
    "        start_pos[2:end] .-= mid_point\n",
    "        for i in 2:length(start_pos)\n",
    "            if start_pos[i] == end_pos[i-1]\n",
    "                start_pos[i] += 1\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # submit jobs\n",
    "        for (s, e) in zip(start_pos, end_pos)\n",
    "            LDfile = joinpath(outdir, \"chr$chr\", \"LD_start$(s)_end$(e).h5\")\n",
    "            summary_file = joinpath(outdir, \"chr$chr\",\"summary_start$(s)_end$(e).csv\")\n",
    "            info_file = joinpath(outdir, \"chr$chr\",\"Info_start$(s)_end$(e).csv\")\n",
    "            job = \"chr$(chr)s$(s)e$e\"\n",
    "            if !(isfile(LDfile) && isfile(summary_file) && isfile(info_file)) && (job ∉ running_jobs)\n",
    "                rm(LDfile, force=true); rm(summary_file, force=true); rm(info_file, force=true); \n",
    "                cmd = \"$exe --file $plinkfile --chr $chr --start_bp $s --end_bp $e --outdir $outdir --genome-build $hg_build --covfile $covfile\"\n",
    "                submit(cmd, 1, 24, jobname=job, highp=false)\n",
    "                failed += 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    println(\"$pop failed $failed\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fda414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
